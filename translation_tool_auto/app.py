import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from translation_service import TranslationService
import os
from dotenv import load_dotenv
import sqlite3
import numpy as np
import re

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æœ¯è¯­æ£€ç´¢å¢å¼ºç¿»è¯‘å·¥å…·",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "terms.db")

# å®šä¹‰å…¨å±€çš„æœ¯è¯­æ•°æ®å’Œå‘é‡å™¨
@st.cache_resource
def load_terms_data():
    """åŠ è½½æœ¯è¯­æ•°æ®å¹¶ç¼“å­˜"""

    print("=== DB DEBUG START ===")
    print("DB_PATH =", DB_PATH)
    print("Exists:", os.path.exists(DB_PATH))
    print("Size:", os.path.getsize(DB_PATH))

      # ä»¥åªè¯»æ–¹å¼æ‰“å¼€æ•°æ®åº“ï¼Œç¡®ä¿è·¯å¾„é”™è¯¯æ—¶ä¸è‡ªåŠ¨åˆ›å»ºç©ºåº“
    conn = sqlite3.connect(f"file:{DB_PATH}?mode=ro", uri=True)
    cursor = conn.cursor()

    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼ˆé˜²æ­¢ç©ºæ•°æ®åº“ï¼‰
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = [t[0] for t in cursor.fetchall()]
    if "terms" not in tables:
        raise RuntimeError(f"'terms' table not found in database at {DB_PATH}")

    # æ‰§è¡ŒæŸ¥è¯¢
    cursor.execute("SELECT word, definition FROM terms")
    rows = cursor.fetchall()
    conn.close()

    # æ‹†åˆ†æˆä¸¤ä¸ªåˆ—è¡¨è¿”å›
    terms = [r[0] for r in rows]
    definitions = [r[1] for r in rows]
    return terms, definitions

@st.cache_resource
def build_vectorizer(terms):
    """æ„å»ºå‘é‡å™¨å¹¶ç¼“å­˜"""
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 3),
        analyzer='word',
        lowercase=True,
        stop_words='english'
    )
    term_matrix = vectorizer.fit_transform(terms)
    return vectorizer, term_matrix

# é¢„å¤„ç†æŸ¥è¯¢
@st.cache_data
def preprocess_query(query):
    """é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬"""
    query = re.sub(r'\([^)]*\)', '', query)
    query = re.sub(r'[^a-zA-Z\s]', '', query)
    query = query.lower()
    return ' '.join(query.split())

# æ£€ç´¢å‡½æ•°
def retrieve_top_k(query, k=5):
    """åŸºäºä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢Top-Kç›¸å…³æœ¯è¯­"""
    # åŠ è½½ç¼“å­˜çš„æœ¯è¯­æ•°æ®
    terms, definitions = load_terms_data()
    
    # é¢„å¤„ç†æŸ¥è¯¢
    processed_query = preprocess_query(query)
    
    # æ„å»ºå¹¶ç¼“å­˜å‘é‡å™¨
    vectorizer, term_matrix = build_vectorizer(terms)
    
    # å‘é‡åŒ–æŸ¥è¯¢
    query_vector = vectorizer.transform([processed_query])
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = cosine_similarity(query_vector, term_matrix)[0]
    
    # è·å–Top-Kç»“æœ
    top_k_indices = np.argsort(similarities)[::-1][:k]
    
    # æ„å»ºç»“æœ
    results = []
    for idx in top_k_indices:
        term = terms[idx]
        sim = similarities[idx]
        results.append({
            "term": term,
            "similarity": float(sim),
            "definition": definitions[term]
        })
    
    return results

# åˆå§‹åŒ–åº”ç”¨çŠ¶æ€
if "terms_loaded" not in st.session_state:
    st.session_state.terms_loaded = False

# åˆ›å»ºä¾§è¾¹æ 
st.sidebar.header("è®¾ç½®")

# APIå¯†é’¥è®¾ç½®
api_key = st.sidebar.text_input(
    "DeepSeek APIå¯†é’¥",
    type="password",
    placeholder="è¯·è¾“å…¥æ‚¨çš„DeepSeek APIå¯†é’¥",
    value=os.getenv("DEEPSEEK_API_KEY", "")
)

# Top-Kå‚æ•°è®¾ç½®
k_value = st.sidebar.slider(
    "æœ¯è¯­æ£€ç´¢æ•°é‡ (Top-K)",
    min_value=1,
    max_value=20,
    value=5,
    step=1
)

# ä¸»æ ‡é¢˜
st.title("ğŸ” æœ¯è¯­æ£€ç´¢å¢å¼ºç¿»è¯‘å·¥å…·")

# åˆ›å»ºä¸¤åˆ—å¸ƒå±€
col1, col2 = st.columns(2)

with col1:
    st.header("è¾“å…¥")
    # æ–‡æœ¬è¾“å…¥åŒº
    input_text = st.text_area(
        "å¾…ç¿»è¯‘æ–‡æœ¬",
        placeholder="è¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬...",
        height=200
    )
    
    # ç¿»è¯‘æŒ‰é’®
    translate_button = st.button("ç¿»è¯‘", use_container_width=True, type="primary")

with col2:
    st.header("ç¿»è¯‘ç»“æœ")
    # ç¿»è¯‘ç»“æœå±•ç¤ºåŒº
    translation_result = st.empty()

# æ·»åŠ æµ‹è¯•æœ¯è¯­æ£€ç´¢çš„åŠŸèƒ½
st.header("ğŸ” æœ¯è¯­æ£€ç´¢æµ‹è¯•")
test_input = st.text_area(
    "è¾“å…¥æ–‡æœ¬è¿›è¡Œæœ¯è¯­æ£€ç´¢æµ‹è¯•",
    placeholder="ä¾‹å¦‚ï¼šArtificial intelligence (AI) is a tool.",
    height=100
)
test_button = st.button("æµ‹è¯•æ£€ç´¢", use_container_width=True)

# æµ‹è¯•æœ¯è¯­æ£€ç´¢å¤„ç†é€»è¾‘
if test_button and test_input:
    with st.spinner("æ­£åœ¨æ£€ç´¢æœ¯è¯­..."):
        try:
            # æ£€ç´¢ç›¸å…³æœ¯è¯­
            related_terms = retrieve_top_k(test_input, k=k_value)
            
            # ç›´æ¥æ˜¾ç¤ºæ£€ç´¢ç»“æœï¼Œä¸ä½¿ç”¨å ä½ç¬¦
            if related_terms:
                st.success(f"æ‰¾åˆ° {len(related_terms)} ä¸ªç›¸å…³æœ¯è¯­")
                for i, term in enumerate(related_terms, 1):
                    with st.expander(f"{i}. {term['term']} (ç›¸ä¼¼åº¦: {term['similarity']:.4f})"):
                        st.write(term['definition'][:200] + "..." if len(term['definition']) > 200 else term['definition'])
            else:
                st.info("æœªæ‰¾åˆ°ç›¸å…³æœ¯è¯­")
        except Exception as e:
            st.error(f"æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            st.exception(e)

# æœ¯è¯­æ£€ç´¢ç»“æœå±•ç¤ºåŒº
st.header("ç›¸å…³æœ¯è¯­æ£€ç´¢ç»“æœ")

# ç¿»è¯‘å¤„ç†é€»è¾‘
if translate_button and input_text:
    # éªŒè¯APIå¯†é’¥
    if not api_key:
        st.error("è¯·åœ¨ä¾§è¾¹æ è¾“å…¥DeepSeek APIå¯†é’¥")
    else:
        # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
        with st.spinner("æ­£åœ¨ç¿»è¯‘..."):
            try:
                # 1. æ£€ç´¢ç›¸å…³æœ¯è¯­
                related_terms = retrieve_top_k(input_text, k=k_value)
                
                # 2. æ‰§è¡Œå¢å¼ºç¿»è¯‘
                translator = TranslationService(api_key=api_key)
                translated_text = translator.translate(input_text, related_terms)
                
                # 3. æ˜¾ç¤ºç¿»è¯‘ç»“æœ
                with col2:
                    translation_result.markdown(f"**ç¿»è¯‘ç»“æœï¼š**\n\n{translated_text}")
                
                # 4. ç›´æ¥æ˜¾ç¤ºæœ¯è¯­æ£€ç´¢ç»“æœï¼Œä¸ä½¿ç”¨å ä½ç¬¦
                if related_terms:
                    st.success(f"æ‰¾åˆ° {len(related_terms)} ä¸ªç›¸å…³æœ¯è¯­")
                    for i, term in enumerate(related_terms, 1):
                        with st.expander(f"{i}. {term['term']} (ç›¸ä¼¼åº¦: {term['similarity']:.4f})"):
                            st.write(term['definition'][:200] + "..." if len(term['definition']) > 200 else term['definition'])
                else:
                    st.info("æœªæ‰¾åˆ°ç›¸å…³æœ¯è¯­")
            except Exception as e:
                st.error(f"ç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                st.exception(e)

# åº”ç”¨è¯´æ˜
st.sidebar.markdown("---")
st.sidebar.header("å…³äº")
st.sidebar.info(
    "è¿™æ˜¯ä¸€ä¸ªåŸºäºæœ¯è¯­æ£€ç´¢å¢å¼ºçš„ç¿»è¯‘å·¥å…·ï¼Œåˆ©ç”¨ç‰›æ´¥è¯å…¸æœ¯è¯­åº“å’ŒDeepSeekå¤§æ¨¡å‹å®ç°ç²¾å‡†ç¿»è¯‘ã€‚\n\n"+
    "åŠŸèƒ½ç‰¹ç‚¹ï¼š\n"+
    "- åŸºäºè¯è¢‹æ¨¡å‹çš„æœ¯è¯­æ£€ç´¢\n"+
    "- ä½™å¼¦ç›¸ä¼¼åº¦Top-KåŒ¹é…\n"+
    "- å¢å¼ºå‹ç¿»è¯‘Prompt\n"+
    "- DeepSeek APIé›†æˆ\n"+
    "- ç›´è§‚æ˜“ç”¨çš„Webç•Œé¢\n"
)

# é¡µè„šä¿¡æ¯
st.markdown("---")
st.markdown("Â© 2025 æœ¯è¯­æ£€ç´¢å¢å¼ºç¿»è¯‘å·¥å…· | åŸºäºDeepSeek APIå’ŒOxfordè¯å…¸")
